{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b5af9a",
   "metadata": {},
   "source": [
    "ðŸ§  Task 2: Text Chunking, Embeddings, and Vector Store Construction\n",
    "Objective\n",
    "\n",
    "The goal of this task is to prepare the cleaned CFPB consumer complaint narratives for Retrieval-Augmented Generation (RAG).\n",
    "This involves:\n",
    "\n",
    "Chunking long complaint texts into manageable segments\n",
    "\n",
    "Generating dense vector embeddings\n",
    "\n",
    "Storing embeddings in a FAISS vector database for semantic search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564a4421",
   "metadata": {},
   "source": [
    "1. Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50571763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YODAHE\\Videos\\rag-complaint-chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba5c6f",
   "metadata": {},
   "source": [
    "2. Load Preprocessed Complaint Data\n",
    "\n",
    "We load the cleaned and filtered dataset produced in Task 1 and remove any remaining missing narratives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "884d5361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80667, 20)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/processed/filtered_complaints.csv\")\n",
    "\n",
    "df = df.dropna(subset=[\"clean_narrative\"]).reset_index(drop=True)\n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55f299a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Optional: limit dataset size with stratified sampling\n",
    "sample_size = 15000  # or 10000\n",
    "df_sampled, _ = train_test_split(\n",
    "    df,\n",
    "    train_size=sample_size,\n",
    "    stratify=df[\"Product\"],  # ensures balanced distribution across products\n",
    "    random_state=42\n",
    ")\n",
    "df = df_sampled.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4ef17b",
   "metadata": {},
   "source": [
    "3. Text Chunking Strategy\n",
    "Why Chunking Is Necessary\n",
    "\n",
    "Complaint narratives can be very long\n",
    "\n",
    "Embedding models perform best on shorter texts\n",
    "\n",
    "Chunking improves retrieval accuracy and recall in RAG systems\n",
    "\n",
    "We use overlapping word-based chunks to preserve context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4825147d",
   "metadata": {},
   "source": [
    "Chunking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fafb2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=300, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = words[i:i + chunk_size]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663fc6b7",
   "metadata": {},
   "source": [
    "Apply Chunking to All Complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af8ecbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20731"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks = []\n",
    "metadata = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    chunks = chunk_text(row[\"clean_narrative\"])\n",
    "\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append(chunk)\n",
    "        metadata.append({\n",
    "            \"Complaint ID\": row[\"Complaint ID\"],\n",
    "            \"Product\": row[\"Product\"]\n",
    "        })\n",
    "\n",
    "len(all_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b34739dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d9d203",
   "metadata": {},
   "source": [
    "Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3eb84780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 648/648 [08:18<00:00,  1.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20731, 384)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = model.encode(\n",
    "    all_chunks,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7f9b4",
   "metadata": {},
   "source": [
    "Create FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37af2302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20731"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension = embeddings.shape[1]\n",
    "\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "index.ntotal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79143f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "vector_dir = \"data/vector_store\"\n",
    "os.makedirs(vector_dir, exist_ok=True)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, os.path.join(vector_dir, \"complaints_faiss.index\"))\n",
    "\n",
    "# Save metadata\n",
    "pd.DataFrame(metadata).to_csv(\n",
    "    os.path.join(vector_dir, \"complaints_metadata.csv\"),\n",
    "    index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff911916",
   "metadata": {},
   "source": [
    "Save Vector Store & Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "376ca6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"../data/complaints_faiss.index\")\n",
    "\n",
    "pd.DataFrame(metadata).to_csv(\n",
    "    \"../data/complaints_metadata.csv\",\n",
    "    index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93df0f2",
   "metadata": {},
   "source": [
    "Sanity Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6c75137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my card was charged for a total of xxxx dollars which i did n t authorize american express didnt srefund my money \n",
      "\n",
      "the bank opened an annual fee credit card without my permission \n",
      "\n",
      "i signed up for a xxxx xxxxr card with first progress xx xx year the card has a 29 00 annual fee after activation i never activated the card i never received any statements i recently obtained a copy  \n",
      "\n",
      "a credit card from barclaysxxxx xxxx account was opened in my name without my knowledge or consent i did not apply for this credit card and i did not authorize any charges on it i discovered the accou \n",
      "\n",
      "the credit card added intrest to my credit card bill for no reason and my bill was past due all because of that and they wont take the late fee and interest off my bill \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"credit card charged fees I did not authorize\"\n",
    "\n",
    "query_embedding = model.encode([query])\n",
    "distances, indices = index.search(query_embedding, k=5)\n",
    "\n",
    "for i in indices[0]:\n",
    "    print(all_chunks[i][:200], \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
